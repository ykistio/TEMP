# 1. 语言

大模型又称大语言模型(Large Language Models) 语言是人类交流思想,表达情感最自然,最深刻,最方便的工具.

> 语言是继真核细胞之后最伟大的进化成就.
>
> 语言本身就是人类有史以来最大的技术发明
>
> 人类历史尚大部分知识是以语言文字形式记载和流传的.

# 2. 自然语言处理

自然语言指的是人类语言,特指文本符号,非语音信号.

1. 自然语言处理(Natural Language Processing. NLP) 用计算机来理解和生成自然语言的各种理论和方法,属于认知智能.
2. 认知智能是人类和动物的主要区别之一, 需要更强的抽象能力和推理能力.
3. 运算智能(能存储会计算) - 感知智能(能听会说能看会认) - 认知智能(能理解会思考)

# 3. 自然语言处理成为制约人工智能取得更大突破和更广泛应用的瓶颈

1. 深度学习的下一个大的进展应该是让神经网络真正理解文档的内容.
2. 深度学习的下一个前沿课题是自然语言理解.
3. 如果给我10亿美金,我回建造一个NASA级别的自然语言处理研究项目.
4. 下一个十年,懂语言者得天下.

# 4. 自然语言处理的发展历史

1. 小规模专家知识(1950-1990)
2. 浅层机器学习算法(1990-2010)
3. 深度学习(2010-2017)
4. 预训练语言模型(2018-2023)
5. 大模型(2023-2024)
6. 推理(2025-?)

推理(Reasoning)是根据已知的信息,事实,规则或前提,通过一定的思维过程和方法,推到出新的结论,判断或知识的认知活动.它是人类思维和智能的核心组成部分,也是人工智能,科学研究和日常决策的关键能力.

# 5. 预训练语言模型

语言模型学到了什么

1. 知识 (哈工大位于_?)
2. 语义(我在水果店看到了苹果,香蕉,_?)
3. 推理,指代(小明打了小红,然后她_?)
4. 情感(我一定推荐给朋友看,这部电影真是_?)

GPT(Generative Pre-traineed Transformer, OpenAI 2018)

1. 采用语言模型预训练任务
2. 语言模型
   1. 计算一个句子在语言中出现的概率.
   2. 或给定上下文,预测下一个词语出现的概率.

GPT在语言模型的基础上具备的三个创新:

1. 使用建模能力更强的Transformer模型
2. 在目标任务尚精调整个预训练模型
3. 接入的下游任务模型可以非常简单

进入与训练时代:

标注文本 -预训练->语料库 -> 模型 -精调


2020年 1750亿参数,无法精调,原来的路走不通了(我先预训练,然后再针对不同的任务做精调?)

换种思路: 如果不能让模型适应下游的任务,反过来,为不同的任务设计相应的"提示语";

无需训练,即可完成文本生成的任务

### GPT3的不足:

预训练语言模型并不能真正克服深度学习模型鲁棒性,可解释性弱,推理能力缺失的瓶颈,故在深层次语义理解上与人类认知水平想去较远!


# 6. ChatGPT

#### 无监督学习

大规模预训练模型,涌现出推理能力

1. 模型规模足够大(>60B)
2. 预训练数据足够多
3. 在代码数据尚继续预训练

#### 有监督学习

使大模型更好遵循人类指令:指令精调(Instruction Tuning)

1. 将各种任务形式进行统一(指令+输入+输出)
2. 在众多(成千上万)任务的标注数据尚精调语言模型
3. 模型能够处理未见任务(Zero-shot)

https://arxiv.org/pdf/2210.11416.pdf

#### 强化学习

将大模型进一步向人类期望对齐: 人类反馈强化学习(RLHF)

1. 降低人工标注难度
2. 生成的结果更多样
3. 能利用负面的标注结果

# 7. ChatGPT后"百模大战", 为什么DeepSeek会火? 历史流程?

1. 高性价比
2. 开源
3. 推理
4. 性能可与OpenAI的o1模型相当

| DS版本  | 发布时间 | 核心技术                                | 训练数据量 | 训练稳定程度 | 最大参数量    |
| ------- | -------- | --------------------------------------- | ---------- | ------------ | ------------- |
| V1      | 2024.1   | 类LLaMA + SFT + RLHF                    | 2T         | 不稳定       | 67B           |
| V2      | 2024.5   | MoE(更多共享专家) + MLA(多头潜在注意力) | 8T         | 较稳定       | 236B(激活21B) |
| V3      | 2024.12  | 基于bias负载均衡 + MTP(多词元预测)      | 14T        | 稳定         | 671B(激活37B) |
| R1-Zero | 2025.1   | 只使用RL学会退率能力+将RL引入基模型架构 |            | 很稳定       | 671B(激活37B) |
| R1      | 2025.1   | SFT学习推理格式 + RL学习推理能力        |            | 很稳定       | 671B(激活37B) |

#### 核心贡献:

1. 只使用强化学习(RL), 模型自主学习到推理能力,性能接近o1模型
2. 训练,推理速度更快,远超o1类模型,极大节约硬件成本
3. DeepSeek坚持开源精神,开放了R1模型及其蒸馏的子模型

#### 推理采用的核心技术:

思维链(Chain-of-Thought, COT): 一系列中间推理步骤,相当于在求解ati过程中将解题步骤也写出来.

#### R1-Zero核心技术

全新的技术栈:基于结果的极简版强化学习(只将规则获得的准确率作为奖励)

1. 强化学习框架:使用GRPO(Group Relative Policy Optimization)作为强化学习框架
   1. 利用当前策略模型进行多次采样,并使用平均奖励值近似价值函数,从而避免了对价值函数的显示训练,这样做即减少了计算开销,有避免了价值函数训练的困难,提高了模型学习的稳定性.
2. 奖励模式:结果/规则奖励, 奖励分为准确率奖励(accuracy rewards),和格式奖励(format rewards), 确保模型输出正确的答案格式和推理过程.

结果:

性能提升 AIME 2024 的分数 39.2% -> 71.0%

RL不需要复杂的算法,简单的GRPO就够用RL需要大量的数据; R1-Zero训练了8000个步骤,如果每个步骤采样的数据量为1024, 也达到了8M级别, 	RL训练不需要过程奖励,结果奖励足够.

# 8. DeepSeek - R1 提升推理的规范性和泛化性

冷启动:为了解决DeepSeek-R1-Zero的可读性差(如推理格式性差,语言混杂等和训练不稳定的问题, DeepSeek-R1引入少量冷启动数据进行预训练,这些数据CoT样本)

收集方法包括:

1. 使用长CoT实例进行少样本提示
2. 直接提示DeepSeek-R1-Zero模型生成带反思和验证的详细答案
3. 收集DeepSeek-R1-Zero的可读格式输入并经人工标注后处理


多阶段训练:

1. 通过冷启动数据进行监督微调(SFT)
2. 进行推理导向的强化学习(RL)
3. 通过拒绝采样生成新的监督微调数据(SFT)
4. 再次进行强化学习以优化模型在所有场景下的表现(RL)

